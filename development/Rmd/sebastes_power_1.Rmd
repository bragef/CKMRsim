---
title: "Assessing Power of Sebastes Microhaplotypes"
author: "Eric C. Anderson"
date: "April 8, 2016"
output:
  html_document: default
  html_notebook: toc true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction
Anthony did a boatload of bioinformatics on two panels of kelp rockfish GT-seq. Then 
Thomas created a great tool haPLOType, available within 
[this repository](https://github.com/ngthomas/callBayes) for visually identifying microhaplotypes.
Subsequently, Diana curated the loci of Panels 1 and 2, providing me with a list of markers to
keep and ones to toss.  That work is chronicled in Asana
[here](https://app.asana.com/0/109989865625977/109989865625980) and 
[here](https://app.asana.com/0/109989865625977/109989865625982).  Then I used haPLOTtype to 
filter according to:

1. min read coverage = 21
2. min allelic ratio = 0.2
3. Retain only the two most frequent haplotypes

Then I downloaded all those haplotype data to this repository for working on them.

Now I am going to do the power analysis.  This is all happening in
`/development/Rmd/sebastes_power_1.Rmd` within the CKMR-sim directory that I will 
push to GitHub soon.

## Initial maneuvers
Some libraries to have:
```{r libs, warning=FALSE, message=FALSE}
library(dplyr)
library(tidyr)
library(readr)
library(readxl)
library(CKMRsim)
library(ggplot2)
```

Load the haplotype data. Make a big long data frame of it all:
```{r hapdat}
hapcalls <- lapply(1:2, function(x){
  file <- paste("../data/filtered_haplotype_panel_", x, ".csv.gz", sep = "")
  read_csv(file)[,-1] # toss the first column which is useless row numbers
}) %>%
  setNames(c("Panel_1", "Panel_2")) %>%
  bind_rows(., .id = "panel")
```

Have a look at those:
```{r lookhap}
hapcalls
```

Likewise, get Diana's "toss" and "keep" calls and her comments on each locus in a long
data frame:
```{r getdiana}
hapkeeps <- lapply(1:2, function(x){
  file <- paste("../data/panel", x, "_status.xlsx", sep = "")
  read_excel(file) %>%
    setNames(c("locus", "status", "notes"))
}) %>%
  setNames(c("Panel_1", "Panel_2")) %>%
  bind_rows(., .id = "panel")
```

Which looks like this
```{r viewhapkeeps}
hapkeeps
```
And we note that no locus names are duplicated across the two panels:
```{r checkdupes}
any(duplicated(hapkeeps$locus))
```

### Tossing/keeping loci
This is now a simple filtering step.  We only keep loci that are in Diana's "keep" list.
```{r keep-keepers}
keepers <- hapkeeps %>%
  filter(status == "keep") %>%
  select(locus) %>%
  unlist() %>% 
  unname()
```
There are `r length(keepers)` such loci.

And here we reduce our haplotype data to only those:
```{r hapkept}
hapkept <- hapcalls %>%
  filter(locus %in% keepers)
```

### Computing allele frequencies
To compute allele freqs we need to just count up the occurrences of the different types
amongst haplotype.1 and haplotype.2.  So, we need to get them into a single column, and
just for the extra challenge we will keep their read depths there as well.
```{r tidyhaps}
haptidy <- hapkept %>%
  unite(col = hap1, haplotype.1, read.depth.1) %>%
  unite(col = hap2, haplotype.2, read.depth.2) %>%
  gather(key = gene_copy, value = H, hap1, hap2) %>%
  separate(H, into = c("Allele", "read_depth")) %>%
  arrange(panel, locus, Indiv.ID, gene_copy)
```
And that looks like this
```{r viewtidyhaps}
haptidy
```

So, now we just need to compute the frequencies for each of the haplotypes
```{r hapfreqs}
hapfreqs <- haptidy %>%
  group_by(locus, Allele) %>%
  summarise(count = n()) %>%
  mutate(Freq = count / sum(count))
```
And the result looks like this
```{r viewhapfreqs}
hapfreqs
```


## Running it in CKMRsim

### Get it in the right format
First we have to get that data frame in the right format and reindex the markers
and make something that `CKMRsim` is expecting to be able to work with (i.e., it has 
haplotypes in descending frequeny at each locus and it has locus and allele indices
in there). To get loci to
be ordered as they are, I have to throw `Pos` in there, even though they are not 
known to have a position on any Chrom.
```{r prep4ckmr}
mhaps <- hapfreqs %>%
  ungroup() %>%
  mutate(Chrom = "GTseq") %>%
  rename(Locus = locus) %>%
  select(-count) %>%
  mutate(Pos = as.integer(factor(Locus, levels = unique(Locus)))) %>%
  mutate(LocIdx = 0,
         AlleIdx = 0) %>%
  CKMRsim::reindex_markers() %>%
  select(Chrom, Locus, Pos, Allele, LocIdx, AlleIdx, Freq)

```

Here is what that looks like:
```{r viewmhaps}
mhaps
```
That shows us that our data set has `r nrow(mhaps)` distinct alleles in it.  Cool!

While we are at it, let's look at the distribution of the number of alleles across loci:
```{r allenumdist}
mhaps %>%
  group_by(Locus) %>%
  summarise(num_haplotypes = n()) %>%
  group_by(num_haplotypes) %>%
  summarise(num_loci = n())
```
That seems pretty nice and makes a good case for using microhaplotypes.

### Doing the CKMRsim analyses
Now that we have that in the right format we can do the simulations.  Ultimately I will have some wrappers
for doing this easily in CKMRsim.  For now, I am still piecing that together.  Let's do it...

```{r ckmr-first-steps, cache=TRUE}
# do this for Parent-offspring, Full-sibling, Half-sibling, and Unrelated,
# and we will throw monozygotic twin in there just for 
# visually checking assumed genotyped error rates 
mhlist <- long_markers_to_X_l_list(D = mhaps, 
                                   kappa_matrix = kappas[c("MZ", "PO", "FS", "HS", "U"), ])

# add the matrices that account for genotyping error
mhlist2 <- insert_C_l_matrices(mhlist, 
                               snp_err_rates = 0.005,
                               scale_by_num_snps = TRUE)

# do the matrix multiplication that gives the Y_l matrices
mhlist3 <- insert_Y_l_matrices(mhlist2)

# then simulate the Q values 
Qvals <- simulate_and_calc_Q(mhlist3, reps = 10^4)
```

OK, now, we can use those Q values to compute log-likelihood ratio statistics and 
look at the overlap between different distributions.  Let's first just make some
pretty plots of the distibutions.  We will let $\Lambda = \log\frac{Q_r}{Q_U}$
where $r$ will be one of PO, FS, or HS.  We will call it $\Lambda_r$ when the true
relationship is actually $r$, and we will call it $\Lambda_U$ when it is computed 
assuming relationship $r$ but the truth is Unrelated.  So, let's compute those into
some variables.

```{r summary-Q}
rs <- c("PO", "FS", "HS")
names(rs) <- rs
lambdas <- lapply(rs, function(r) {
  data_frame(True = Qvals[[r]][[r]] - Qvals[[r]]$U,
             Unrelated = Qvals$U[[r]] - Qvals$U$U)
}) %>%
  bind_rows(.id = "relat") %>%
  gather(key = "lambda_type", value = "LogL_Ratio", True, Unrelated) %>%
  mutate(relat = factor(relat, levels = c("PO", "FS", "HS")))
```

Let's make a plot of those:
```{r plot-lambdas, cache=TRUE, dependson="ckmr-first-steps"}
ggplot(lambdas, aes(x = LogL_Ratio, fill = lambda_type)) +
  geom_density(alpha = 0.3) +
  facet_wrap(~ relat, ncol = 1)
```

That shows us that we have very good separation for Parent-Offspring and decent
separation for Full Siblings, and not very good separation on Half-Siblings.  

In order to get a really good sense for what the false positive rates are going to be 
we will need to do importance sampling using the Qvals.  I haven't yet implemented this 
in a function, but will just do it hastily here.



### Importance sampling
We have a function to do importance sampling and vanilla monte carlo:
```{r mhaps-mc}
mhaps_mc <- mc_sample_simple(Qvals, nu = c("PO", "FS", "HS"), method = "both") %>% as.data.frame
mhaps_mc
```

So, it looks like PO should no problem with these markers.  FS pairs at kelp rockfish is probably quite
doable, but this has assumed no physical linkage, so those FPRs are a little optimistic for the FS case.
But, as long as we are willing to tolerate false negative rates on the order of 10% FS probably will be 
OK. Doing FS in other species that might be harder---it depends on how many of these markers
translate over to those other species.

At any rate, these results are pretty encouraging.

### A digression on assumed genotyping error rates
I just wanted to point out that I cobbled together an error model for these that includes 
a chance at allelic dropout and also a chance at sequencing errors.  The probability of
correctly calling a genotype, broken down by the number of haplotypes at a locus, is 
shown in the following plot.
```{r correct-call-rates}
calls <- lapply(mhlist2, function(x) data.frame(nA = length(x$freqs), rate_correct = diag(x$C_l))) %>%
  bind_rows(.id = "Locus")
ggplot(calls, aes(x = rate_correct)) +
  geom_density() +
  facet_wrap(~ nA, ncol = 3)
```



## Investigating half siblings with more markers
This is a sort of artificial exercise because it assumes no physical linkage, but the
question arose whether it would take just a few more or a whole lot more markers to 
identify half siblings.  So we can at least get a lower bound here. 

Here is what we will do: duplicate the 165 markers 16 times, then do the calcuations for varying
numbers of markers. Notice that the microhap data is now included in the R package as
`sebastes`.
```{r rep-and-prep}
numAlles <- nrow(sebastes)  # the number of alleles we are dealing with here

# here we replicate the loci and reindex them
repped <- lapply(1:16, function(x) sebastes) %>% bind_rows(.id = "rep") %>%
  mutate(Locus = paste("Rep-", rep, "-", Locus, sep = ""),
         Pos = 1000 * (as.integer(rep) - 1)  + Pos
         ) %>%
  select(-rep) %>%
  reindex_markers()
  
```

Now, we just have to compute the Qvals with different numbers of markers.  We are only going to
do it for HS and U:
```{r repped-qvals, cache=TRUE}
numRows <- c(1, 2, 4, 8, 16) * numAlles
names(numRows) <- c(1, 2, 4, 8, 16) * 165  # named by the number of loci

QvList <- lapply(numRows, function(x) {
  long_markers_to_X_l_list(D = repped[1:x, ], kappa_matrix = kappas[c("HS", "U"), ]) %>%
    insert_C_l_matrices(., snp_err_rates = 0.005, scale_by_num_snps = TRUE) %>%
    insert_Y_l_matrices() %>%
    simulate_and_calc_Q(., reps = 10^4)
})
```

And finally, get the samples:
```{r repped-sampling, cache=TRUE}
repped_fprs <- lapply(QvList, function(Q) {
  mc_sample_simple(Q, "HS")
}) %>% 
  bind_rows(.id = "num_loci")

repped_fprs
```

Let's look at all the values:
```{r look-at-em}
as.data.frame(repped_fprs)
```


## Comparing the results to the "best" SNP from each fragment
For this next part, we are going to explode those haplotypes into constituent SNPs 
and estimate their frequencies, and then take the best SNPs from each microhaplotype
and see how well it works.  We are going to operate on `mhaps` for this and then 
get a data frame called `best_snps` which we will then pass off to CKMRsim.
```{r get-best-snps}
# get all the SNP freqs
snp_freqs <- mhaps %>%
  split(f = mhaps$Locus) %>%
  lapply(function(x) {
    x$exploded = sapply(strsplit(x$Allele, split = ""), function(y) paste(y, collapse = "."))
    x
  }) %>%
  lapply(., function(x) {
    separate(x, exploded, into = paste("snp", 1:nchar(x$Allele[1]), sep = "_"))
  }) %>%
  lapply(., function(x) {
    gather(x, key = "SNP", value = "base", contains("snp_"))
  }) %>%
  bind_rows %>%
  group_by(Chrom, Locus, Pos, LocIdx, SNP, base) %>% 
  summarise(Freq = sum(Freq))

# now, get the best (MAF closest to 0.5)
best_snps <- snp_freqs %>%
  group_by(Locus, SNP) %>%
  filter(n() > 1) %>%  # toss SNPs that are monomorphic---for some reason there are some...
  mutate(maf = min(Freq)) %>%
  group_by(Locus) %>%
  filter(maf == max(maf))  %>%  # this almost does it, but some snps at a locus might have the same MAF at different snps
  mutate(tmp = 1:n()) %>%
  filter(tmp < 3)  %>% # this gets rid of those same MAF cases.
  select(-tmp, -maf) %>%
  rename(Allele = base) %>%
  mutate(AlleIdx = 0) %>%
  CKMRsim::reindex_markers() %>%
  select(Chrom, Locus, Pos, Allele, LocIdx, AlleIdx, Freq)

```
So, that rather exciting set of operations got us our best SNPs from each of the 165 fragments.  
There are 165 of them, just like there should
be.  So, now we just need to run CKMR sim on them:
```{r best-snps-ckmr-sim, cache = TRUE}
# do this for Parent-offspring, Full-sibling, Half-sibling, and Unrelated,
# and we will throw monozygotic twin in there just for 
# visually checking assumed genotyped error rates 
bslist <- long_markers_to_X_l_list(D = best_snps, 
                                   kappa_matrix = kappas[c("MZ", "PO", "FS", "HS", "U"), ])

# add the matrices that account for genotyping error
bslist2 <- insert_C_l_matrices(bslist, 
                               snp_err_rates = 0.005,
                               scale_by_num_snps = TRUE)

# do the matrix multiplication that gives the Y_l matrices
bslist3 <- insert_Y_l_matrices(bslist2)

# then simulate the Q values 
Qvals_bs <- simulate_and_calc_Q(bslist3, reps = 10^4)

```
We can now visually compare the distributions those from all the microhaplotypes.  We will toss HS out of the
mix because it is not so interesting.
```{r compare-snps-mhaps-densities, fig.width=14, fig.height=10}
# gotta make a function in CKMRsim for this sort of operation
rs <- c("PO", "FS", "HS")
names(rs) <- rs
lambdas_bs <- lapply(rs, function(r) {
  data_frame(True = Qvals_bs[[r]][[r]] - Qvals_bs[[r]]$U,
             Unrelated = Qvals_bs$U[[r]] - Qvals_bs$U$U)
}) %>%
  bind_rows(.id = "relat") %>%
  gather(key = "lambda_type", value = "LogL_Ratio", True, Unrelated) %>%
  mutate(relat = factor(relat, levels = c("PO", "FS", "HS")))

ffp <- bind_rows(list(micro_haps = lambdas, SNPs = lambdas_bs), .id = "data_type") %>%
  filter(relat != "HS") %>%
  droplevels()

ggplot(ffp, aes(x = LogL_Ratio, fill = lambda_type)) +
  geom_density(alpha = 0.3) +
  facet_grid(data_type ~ relat)

# also, save that dude as a pdf
ggsave(filename = "../outputs/snps_vs_mhaps_165.pdf", width = 7, height = 4)
```
And now we can do the MC sampling on that to get the false positive rates.
```{r mc-sample-best-snps}
mc_sample_simple(Qvals_bs, nu = c("PO", "FS", "HS"), method = "both") %>% as.data.frame()
```


## Doing the best 96 loci
First we are going to find the microhaps with the highest heterozygosity, and the SNPs with the high MAF
```{r high-het-mhaps}
mhap_hz <- mhaps %>%
  group_by(Locus) %>% 
  summarise(hz = 1 - sum(Freq^2), nHaps = n()) %>%
  arrange(desc(hz)) 

snp_maf <- best_snps %>%
  group_by(Locus) %>%
  arrange(Freq) %>%
  summarise(MAF = first(Freq)) %>%
  ungroup() %>%
  arrange(desc(MAF))
```

After looking at those, I think it will be fine to just focus on the 96 best SNPs.  So, let us do that:
```{r best-snp-stuff}
### First the mhaps
mhlist96 <- long_markers_to_X_l_list(D = mhaps %>% filter(Locus %in% snp_maf$Locus[1:96]), 
                                   kappa_matrix = kappas[c("PO", "FS", "HS", "U"), ])

# add the matrices that account for genotyping error
mhlist96_2 <- insert_C_l_matrices(mhlist96, 
                               snp_err_rates = 0.005,
                               scale_by_num_snps = TRUE)

# do the matrix multiplication that gives the Y_l matrices
mhlist96_3 <- insert_Y_l_matrices(mhlist96_2)

# then simulate the Q values 
Qvals_96 <- simulate_and_calc_Q(mhlist96_3, reps = 10^4)

### Then the 96 SNPs
bslist96 <- long_markers_to_X_l_list(D = best_snps %>% filter(Locus %in% snp_maf$Locus[1:96]), 
                                   kappa_matrix = kappas[c("PO", "FS", "HS", "U"), ])

# add the matrices that account for genotyping error
bslist96_2 <- insert_C_l_matrices(bslist96, 
                               snp_err_rates = 0.005,
                               scale_by_num_snps = TRUE)

# do the matrix multiplication that gives the Y_l matrices
bslist96_3 <- insert_Y_l_matrices(bslist96_2)

# then simulate the Q values 
Qvals_bs96 <- simulate_and_calc_Q(bslist96_3, reps = 10^4)
```
Now just do the mc sampling:
```{r top-96-mc-sample}
compare96 <- lapply(list(mhaps = Qvals_96, snps = Qvals_bs96), function(x)
  mc_sample_simple(Q = x, nu = c("PO", "FS", "HS"), method = "both")
  ) %>% bind_rows(.id = "data_type")
as.data.frame(compare96)
```
Wow! The 96 SNPs really blow chunks tremendously!

Let's draw the figure:
```{r plot-dsns-96}
rs <- c("PO", "FS", "HS")
names(rs) <- rs
lambdas_bs96 <- lapply(rs, function(r) {
  data_frame(True = Qvals_bs96[[r]][[r]] - Qvals_bs96[[r]]$U,
             Unrelated = Qvals_bs96$U[[r]] - Qvals_bs96$U$U)
}) %>%
  bind_rows(.id = "relat") %>%
  gather(key = "lambda_type", value = "LogL_Ratio", True, Unrelated) %>%
  mutate(relat = factor(relat, levels = c("PO", "FS", "HS")))

lambdas_96 <- lapply(rs, function(r) {
  data_frame(True = Qvals_96[[r]][[r]] - Qvals_96[[r]]$U,
             Unrelated = Qvals_96$U[[r]] - Qvals_96$U$U)
}) %>%
  bind_rows(.id = "relat") %>%
  gather(key = "lambda_type", value = "LogL_Ratio", True, Unrelated) %>%
  mutate(relat = factor(relat, levels = c("PO", "FS", "HS")))

ffp96 <- bind_rows(list(micro_haps = lambdas_96, SNPs = lambdas_bs96), .id = "data_type")

ggplot(ffp96 %>% filter(relat != "HS"), aes(x = LogL_Ratio, fill = lambda_type)) +
  geom_density(alpha = 0.3) +
  facet_grid(data_type ~ relat)
```
Cool! That's a wrap.  It is looking good!
